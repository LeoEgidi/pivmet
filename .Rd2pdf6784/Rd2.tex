\documentclass[a4paper]{book}
\usepackage[times,inconsolata,hyper]{Rd}
\usepackage{makeidx}
\usepackage[utf8]{inputenc} % @SET ENCODING@
% \usepackage{graphicx} % @USE GRAPHICX@
\makeindex{}
\begin{document}
\chapter*{}
\begin{center}
{\textbf{\huge Package `pivmet'}}
\par\bigskip{\large \today}
\end{center}
\begin{description}
\raggedright{}
\inputencoding{utf8}
\item[Type]\AsIs{Package}
\item[Title]\AsIs{Pivotal methods for Bayesian relabelling and k-means clustering}
\item[Version]\AsIs{0.1.0}
\item[Author]\AsIs{Leonardo Egidi}
\item[Maintainer]\AsIs{Leonardo Egidi }\email{legidi@units.it}\AsIs{}
\item[Description]\AsIs{The package provides some pivotal algorithms
for dealing with Bayesian Gaussian mixture models and relabelling
the MCMC chains in order to undo the label switching problem.
The same pivotal methods may be used for initializing the
centers of the classical k-means algorithm in order to
obtain a better clustering solution.}
\item[url]\AsIs{https://github.com/leoegidi/pivmet}
\item[Encoding]\AsIs{UTF-8}
\item[LazyData]\AsIs{true}
\item[LazyLoad]\AsIs{yes}
\item[Imports]\AsIs{mvtnorm, bayesmix, RcmdrMisc,
cluster, mclust, runjags, rjags, MASS}
\item[Depends]\AsIs{R, mvtnorm, bayesmix, RcmdrMisc,
cluster, mclust, runjags, rjags, MASS}
\item[Suggests]\AsIs{knitr}
\item[VignetteBuilder]\AsIs{knitr}
\item[RemoteType]\AsIs{github}
\item[RemoteHost]\AsIs{https://api.github.com}
\item[RemoteRepo]\AsIs{pivmet}
\item[RemoteUsername]\AsIs{LeoEgidi}
\item[RemoteRef]\AsIs{master}
\item[RemoteSha]\AsIs{59e8616331302cc911ae13b6190580dcfee099b9}
\item[GithubRepo]\AsIs{pivmet}
\item[GithubUsername]\AsIs{LeoEgidi}
\item[GithubRef]\AsIs{master}
\item[GithubSHA1]\AsIs{59e8616331302cc911ae13b6190580dcfee099b9}
\item[RoxygenNote]\AsIs{6.1.0}
\item[BuildManual]\AsIs{yes}
\end{description}
\Rdcontents{\R{} topics documented:}
\inputencoding{utf8}
\HeaderA{MUS}{MUS algorithm}{MUS}
%
\begin{Description}\relax
Perform Maxima Units Search (MUS) algorithm on a large and sparse matrix in
order to find a set of pivotal units through a sequential search
in the given matrix.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
MUS(C, clusters, prec_par = 5)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{C}] \eqn{N \times N}{} matrix with a non-negligible number of zeros.
For instance, a similarity matrix estimated from a \code{NxD} data matrix whose rows
are statistical units, or a co-association matrix resulting from clustering
ensembles.

\item[\code{clusters}] A vector of integers from \code{1:k} (with \code{k <= 4})
indicating a partition of the \eqn{N}{} units resulting from clustering.

\item[\code{prec\_par}] Optional argument. The maximum number of alternative pivots for each group.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Consider \eqn{H}{} distinct partitions of a set of \eqn{N}{} \eqn{d}{}-dimensional
statistical units into \eqn{k}{}
groups determined by some
clustering technique.  A \eqn{N \times N}{} co-association matrix
\eqn{C}{} with generic element \eqn{c_{i,j}=n_{i,j}/H}{} can be constructed,
where \eqn{n_{i,j}}{} is the number of times the \eqn{i}{}-th and the \eqn{j}{}-th unit
are assigned to the same cluster with respect to the clustering ensemble.
Units which are very distant
from each other are likely to have zero co-occurrences; as a consequence,
\eqn{C}{} is
a square symmetric matrix expected  to contain a non-negligible number of zeros.
The main task of the MUS algorithm is to detect submatrices of small
rank from the co-association matrix
and extract those units---pivots---such
that the \eqn{k \times k}{} submatrix of \eqn{C}{},
determined by only the pivotal rows
and columns indexes, is identical or nearly identical.
Practically, the resulting units
have the desirable property to be representative of
the group they belong to.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{\code{pivots}}]  The \code{k} pivotal units
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Egidi, L., Pappadà, R., Pauli, F., Torelli, N. (2018).
Maxima Units Search(MUS) algorithm:
methodology and applications. In: Perna, C. , Pratesi, M., Ruiz-Gazen A. (eds.) Studies in
Theoretical and Applied Statistics,
Springer Proceedings in Mathematics and Statistics 227, pp. 71–81.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
# Data generated from a mixture of three bivariate Gaussian distributions

N <- 620
centers  <- 3
n1 <- 20
n2 <- 100
n3 <- 500
x  <- matrix(NA, N,2)
truegroup <- c( rep(1,n1), rep(2, n2), rep(3, n3))

for (i in 1:n1){
 x[i,]=rmvnorm(1, c(1,5), sigma=diag(2))}
for (i in 1:n2){
 x[n1+i,]=rmvnorm(1, c(4,0), sigma=diag(2))}
for (i in 1:n3){
 x[n1+n2+i,]=rmvnorm(1, c(6,6), sigma=diag(2))}

# Build a similarity matrix from clustering ensembles

H <- 1000
a <- matrix(NA, H, N)

for (h in 1:H){
   a[h,] <- kmeans(x,centers)$cluster
}

sim_matr <- matrix(1, N,N)
for (i in 1:(N-1)){
  for (j in (i+1):N){
     sim_matr[i,j] <- sum(a[,i]==a[,j])/H
     sim_matr[j,i] <- sim_matr[i,j]
     }
}

# Obtain a clustering solution via kmeans with multiple random seeds

cl <- KMeans(x, centers)$cluster

# Find three pivots

mus_alg <- MUS(C = sim_matr, clusters = cl)


\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{piv\_KMeans}{k-means Clustering Using Pivotal Algorithms For Seeding}{piv.Rul.KMeans}
%
\begin{Description}\relax
Perform classical k-means clustering on a data matrix using pivots as
initial centers.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
piv_KMeans(x, centers, alg.type = c("KMeans", "hclust"),
  piv.criterion = c("MUS", "maxsumint", "maxsumnoint", "maxsumdiff"),
  H = 1000, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A \eqn{N \times D}{} data matrix, or an object that can be coerced to such a matrix (such as a numeric vector or a dataframe with all numeric columns).

\item[\code{centers}] The number of clusters in the solution.

\item[\code{alg.type}] The clustering algorithm for the initial partition of the
\eqn{N}{} units into the desired number of clusters.
Possible choices are \code{"KMeans"} and \code{"hclust"}.

\item[\code{piv.criterion}] The pivotal criterion used for identifying one pivot
for each group. Possible choices are: \code{"MUS", "maxsumint", "maxsumnoint",
"maxsumdiff"}.
If \code{centers <= 4}, the default method is \code{"MUS"};
otherwise, the default method is \code{"maxsumdiff"} (see the details and
the vignette).

\item[\code{H}] If \code{"MUS"} is selected, this is the number of
distinct k-means partitions used for building a \eqn{N \times N}{}
co-association matrix.

\item[\code{...}] Optional arguments to be passed to \code{MUS} or \code{KMeans}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The function implements a modified version of k-means which aims at
improving the clustering solution starting from a careful seeding.
In particular, it performs a pivot-based initialization step
using pivotal methods to find the initial centers
for the clustering procedure. The starting point consists of multiple
runs of the classical k-means (which uses random seeds via \code{Kmeans}
function of the \code{RcmdrMisc} package)
with a fixed number of clusters
in order to build the co-association matrix of data units.
\end{Details}
%
\begin{Value}
A list with components

\begin{ldescription}
\item[\code{\code{cluster}}] A vector of integers indicating the cluster to which each point is allocated.
\item[\code{\code{centers}}] A matrix of cluster centres (centroids).
\item[\code{\code{totss}}] The total sum of squares.
\item[\code{\code{withinss}}] The within-cluster sum of squares for each cluster.
\item[\code{\code{tot.withinss}}] The within-cluster sum of squares summed across clusters.
\item[\code{\code{betwennss}}] The between-cluster sum of squared distances.
\item[\code{\code{size}}]  The number of points in each cluster.
\item[\code{\code{iter}}] The number of (outer) iterations.
\item[\code{\code{ifault}}] integer: indicator of a possible algorithm problem – for experts.
\item[\code{\code{pivots}}] The pivotal units identified by the selected pivotal criterion.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Leonardo Egidi \url{legidi@units.it}
\end{Author}
%
\begin{References}\relax
Egidi, L., Pappadà, R., Pauli, F., Torelli, N. (2018).
K-means seeding via MUS algorithm. Conference Paper,
Book of Short Papers, SIS2018, ISBN: 9788891910233.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}

# Data generated from a mixture of three bivariate Gaussian distributions

N  <- 620
k  <- 3
n1 <- 20
n2 <- 100
n3 <- 500
x  <- matrix(NA, N,2)
truegroup <- c( rep(1,n1), rep(2, n2), rep(3, n3))

for (i in 1:n1){
 x[i,]=rmvnorm(1, c(1,5), sigma=diag(2))}
for (i in 1:n2){
 x[n1+i,]=rmvnorm(1, c(4,0), sigma=diag(2))}
for (i in 1:n3){
 x[n1+n2+i,]=rmvnorm(1, c(6,6), sigma=diag(2))}

# Apply piv_KMeans with MUS as pivotal criterion

res <- piv_KMeans(x, k)

# Apply piv_KMeans with maxsumdiff as pivotal criterion

res2 <- piv_KMeans(x, k, piv.criterion ="maxsumdiff")

# Plot the data and the clustering solution

par(mfrow=c(1,2), pty="s")
colors_cluster <- c("grey", "darkolivegreen3", "coral")
colors_centers <- c("black", "darkgreen", "firebrick")
plot(x, col = colors_cluster[truegroup],
   bg= colors_cluster[truegroup], pch=21, xlab="x[,1]",
   ylab="x[,2]", cex.lab=1.5,
   main="True data", cex.main=1.5)

plot(x, col = colors_cluster[res$cluster],
   bg=colors_cluster[res$cluster], pch=21, xlab="x[,1]",
   ylab="x[,2]", cex.lab=1.5,
   main="piv_KMeans", cex.main=1.5)
points(x[res$pivots[1],1], x[res$pivots[1],2],
   pch=24, col=colors_centers[1],bg=colors_centers[1],
   cex=1.5)
points(x[res$pivots[2],1], x[res$pivots[2],2],
   pch=24,  col=colors_centers[2], bg=colors_centers[2],
   cex=1.5)
points(x[res$pivots[3],1], x[res$pivots[3],2],
   pch=24, col=colors_centers[3], bg=colors_centers[3],
   cex=1.5)
points(res$centers, col = colors_centers[1:k],
   pch = 8, cex = 2)
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{piv\_MCMC}{JAGS Sampling for Gaussian Mixture Models and Clustering via Co-Association Matrix.}{piv.Rul.MCMC}
%
\begin{Description}\relax
Perform MCMC JAGS sampling for Gaussian mixture models, post-process the chains and apply a clustering technique to the MCMC sample. Pivotal units for each group are selected among four alternative criteria.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
piv_MCMC(y, k, priors, nMC, piv.criterion = c("MUS", "maxsumint",
  "maxsumnoint", "maxsumdiff"), clustering = c("diana", "hclust"))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{y}] N-dimensional data vector/matrix.

\item[\code{k}] Number of mixture components.

\item[\code{priors}] Input prior hyperparameters (see Details).

\item[\code{nMC}] Number of MCMC iterations for the JAGS function execution.

\item[\code{piv.criterion}] The pivotal criterion used for identifying one pivot
for each group. Possible choices are: \code{"MUS", "maxsumint", "maxsumnoint",
"maxsumdiff"}.
If \code{k <= 4}, the default method is \code{"MUS"};
otherwise, the default method is \code{"maxsumdiff"} (see the Details and
the vignette).

\item[\code{clustering}] The clustering technique adopted for partitioning the
\code{N} observations into \code{k} groups. Possible choices: \code{"diana"} (default),
\code{"hclust"}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The function fits univariate and bivariate Bayesian Gaussian mixture models of the form
(here for univariate only):
\deqn{(Y_i|Z_i=j) \sim \mathcal{N}(\mu_j,\phi_j),}{}
where the \eqn{Z_i}{}, \eqn{i=1,\ldots,N}{}, are i.i.d. random variables, \eqn{j=1,\dots,k}{},
\eqn{\phi_j}{} is the group variance,  \eqn{Z_i \in {1,\ldots,k }}{} are the
latent group allocation, and
\deqn{P(Z_i=j)=\pi_j.}{}
The likelihood of the model is then
\deqn{L(y;\mu,\pi,\phi) = \prod_{i=1}^N \sum_{j=1}^k \pi_j \mathcal{N}(\mu_j,\phi_j),}{}
where \eqn{(\mu, \phi)=(\mu_{1},\dots,\mu_{k},\phi_{1},\ldots,\phi_{k})}{}
are the component-specific parameters and \eqn{\pi=(\pi_{1},\dots,\pi_{k})}{}
the mixture weights. Let \eqn{\nu}{} denote a permutation of \eqn{{ 1,\ldots,k }}{},
and let \eqn{\nu(\mu)= (\mu_{\nu(1)},\ldots,}{} \eqn{ \mu_{\nu(k)})}{},
\eqn{\nu(\phi)= (\phi_{\nu(1)},\ldots,}{} \eqn{ \phi_{\nu(k)})}{},
\eqn{ \nu(\pi)=(\pi_{\nu(1)},\ldots,\pi_{\nu(k)})}{} be the
corresponding permutations of \eqn{\mu}{}, \eqn{\phi}{} and \eqn{\pi}{}.
Denote by \eqn{V}{} the set of all the permutations of the indexes
\eqn{{1,\ldots,k }}{}, the likelihood above is invariant under any
permutation \eqn{\nu \in V}{}, that is
\deqn{
L(y;\mu,\pi,\phi) = L(y;\nu(\mu),\nu(\pi),\nu(\phi)).}{}
As a consequence, the model is unidentified with respect to an
arbitrary permutation of the labels.
When Bayesian inference for the model is performed,
if the prior distribution \eqn{p_0(\mu,\pi,\phi)}{} is invariant under a permutation of the indices, then so is the posterior. That is, if \eqn{p_0(\mu,\pi,\phi) = p_0(\nu(\mu),\nu(\pi),\phi)}{}, then
\deqn{
p(\mu,\pi,\phi| y) \propto p_0(\mu,\pi,\phi)L(y;\mu,\pi,\phi)}{}
is multimodal with (at least) \eqn{k!}{} modes.

Priors are chosen as weakly informative. For univariate mixtures,
the specification is the same as the function \code{BMMmodel} of the
\code{bayesmix} package:

\deqn{\mu_j \sim \mathcal{N}(0, 1/B0inv)}{}
\deqn{\phi_j \sim \mbox{invGamma}(nu0Half, nu0S0Half)}{}
\deqn{\pi \sim \mbox{Dirichlet}(1,\ldots,1)}{}
\deqn{S0 \sim \mbox{Gamma}(g0Half, g0G0Half),}{}

with default values: \eqn{B0inv=0.1, nu0Half =10, S0=2,
 nu0S0Half= nu0Half\times S0,
 g0Half = 5e-17, g0G0Half = 5e-33}{}, in accordance with the default
specification:

\code{priors=list(kind = "independence", parameter = "priorsFish",
 hierarchical = "tau")}

(see \code{bayesmix} for further details and choices).

For bivariate mixtures, the prior specification is the following:

\deqn{ \bm{\mu}_j  \sim \mathcal{N}_2(\bm{\mu}_0, S2)}{}
\deqn{ 1/\Sigma \sim \mbox{Wishart}(S3, 3)}{}
\deqn{\pi \sim \mbox{Dirichlet}(1,\ldots,1),}{}

where \eqn{S2}{} and \eqn{S3}{} are diagonal matrices
with diagonal elements (the variances)
equal to 1e+05. The user may specify other values for the hyperparameters
\eqn{\bm{\mu}_0, S2, S3}{} via \code{priors} argument in such a way:

\code{priors =list(mu0 = c(1,1), S2 = ...,S3 = ...)},

with the constraint for \eqn{S2}{} and \eqn{S3}{} to be positive definite.

The function performs JAGS sampling using the \code{bayesmix} package for univariate Gaussian mixtures, and the \code{runjags}
package for bivariate Gaussian mixtures. After MCMC sampling, this function
clusters the units in \code{k} groups,
calls the \code{piv\_sel()} function and yields the
pivots obtained from one among four different
methods (the user may specify one among them via \code{piv.criterion}
argument):
\code{"maxsumint"}, \code{"maxsumnoint"}, \code{"maxsumdiff"}
and \code{"MUS"} (available only if \code{k <= 4})
(see the vignette for thorough details). Due to computational reasons
clarified in the Details section of the function \code{piv\_rel}, the
length of the MCMC chains will be minor or equal the input
argument \code{nMC}; this length, corresponding to the value
\code{true.iter} returned by the procedure, is the number of
MCMC iterations for which
the number of JAGS groups exactly coincides with the prespecified
number of groups \code{k}.
\end{Details}
%
\begin{Value}
The function gives the MCMC output, the clustering
solutions and the pivotal indexes. Here there is a complete list of outputs.

\begin{ldescription}
\item[\code{\code{Freq}}]   \code{k x 2} matrix where: the first column
reports the number of units allocated to each group
as given by JAGS program; the second
column reports the same number of units as given by the
chains' post-processing.
\item[\code{\code{true.iter}}]  The number of MCMC iterations for which
the number of JAGS groups exactly coincides with the prespecified
number of groups \code{k}.
\item[\code{\code{z} }]   \code{N x k x true.iter} array with values: 1,
if the \eqn{i}{}-th unit belongs to the \eqn{j}{}-th group at
the \eqn{h}{}-th iteration; 0, otherwise.
\item[\code{\code{ris}}]   MCMC output matrix as provided by JAGS.
\item[\code{\code{groupPost}}]  \code{true.iter x N} matrix
with values from \code{1:k} indicating the post-processed group allocation
vector.
\item[\code{\code{mu\_switch}}]   If \code{y} is a vector, a \code{true.iter x k}
matrix with the post-processed MCMC chains for the mean parameters; if
\code{y} is a matrix, a \code{true.iter x 2 x k} array with
the post-processed MCMC chains for the mean parameters.
\item[\code{\code{mu\_raw}}]  If \code{y} is a vector, a \code{nMC x k} matrix
with the raw MCMC chains for the mean parameters as given by JAGS; if
\code{y} is a matrix, a \code{nMC x 2 x k} array with the raw MCMC chains
for the mean parameters as given by JAGS.
\item[\code{\code{C}}] Co-association matrix constructed from the MCMC sample.
\item[\code{\code{grr}}] Group vector allocation as provided by
\code{"diana"} or \code{"hclust"}.
\item[\code{\code{pivots}}]  The pivotal units identified by the
selected pivotal criterion.
\item[\code{\code{piv.criterion}}]  Gives the pivotal criterion used for identifying
the pivots.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Leonardo Egidi \url{legidi@units.it}
\end{Author}
%
\begin{References}\relax
Egidi, L., Pappadà, R., Pauli, F. and Torelli, N. (2018). Relabelling in Bayesian Mixture
Models by Pivotal Units. Statistics and Computing, 28(4), 957-969.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}

# Bivariate simulation

N   <- 200
k   <- 4
nMC <- 1000
M1  <-c(-.5,8)
M2  <- c(25.5,.1)
M3  <- c(49.5,8)
M4  <- c(63.0,.1)
Mu  <- matrix(rbind(M1,M2,M3,M4),c(4,2))
stdev    <- cbind(rep(1,k), rep(200,k))
Sigma.p1 <- matrix(c(stdev[1,1],0,0,stdev[1,1]), nrow=2, ncol=2)
Sigma.p2 <- matrix(c(stdev[1,2],0,0,stdev[1,2]), nrow=2, ncol=2)
W <- c(0.2,0.8)
sim <- piv_sim(N,k,Mu, stdev, Sigma.p1,Sigma.p2,W)
res <- piv_MCMC(y = sim$y, k =k, nMC = nMC)
#changing priors
res2 <- piv_MCMC(y = sim$y,
                 priors = list (
                 mu0=c(1,1),
                 S2 = matrix(c(0.002,0,0, 0.1),2,2, byrow=TRUE),
                 S3 = matrix(c(0.1,0,0,0.1), 2,2, byrow =TRUE)),
                 k = k, nMC = nMC)



# Fishery data (bayesmix package)

data(fish)
y <- fish[,1]
k <- 5
nMC <- 5000
res <- piv_MCMC(y = y, k = k, nMC = nMC)
# changing priors
res2   <- piv_MCMC(y = y,
                   priors = list(kind = "condconjugate",
                   parameter = "priorsRaftery",
                   hierarchical = "tau"),  k =k, nMC = nMC)

\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{piv\_plot}{Plotting outputs from pivotal relabelling}{piv.Rul.plot}
%
\begin{Description}\relax
Plot and visualize MCMC outputs, posterior relabelled chains and estimates and diagnostics.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
piv_plot(y, mcmc, rel_est, type = c("chains", "estimates", "hist"))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{y}] Data vector or matrix.

\item[\code{mcmc}] The ouptut of the raw MCMC sampling, as provided by \code{piv\_MCMC}.

\item[\code{rel\_est}] Pivotal estimates as provided by \code{piv\_rel}.

\item[\code{type}] Type of plots required. Choose among: \code{"chains"}, \code{"estimates"}, \code{"hist"}.
\end{ldescription}
\end{Arguments}
%
\begin{Author}\relax
Leonardo Egidi \url{legidi@units.it}
\end{Author}
%
\begin{Examples}
\begin{ExampleCode}

# Fishery data

data(fish)
y <- fish[,1]
N <- length(y)
k <- 5
nMC <- 5000
res <- piv_MCMC(y = y, k = k, nMC = nMC)
rel <- piv_rel(mcmc=res, nMC = nMC)
piv_plot(y, res, rel, "chains")
piv_plot(y, res, rel, "estimates")
piv_plot(y, res, rel, "hist")

\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{piv\_rel}{Perfroming the pivotal relabelling step and computing the relabelled posterior estimates}{piv.Rul.rel}
%
\begin{Description}\relax
This function allows to perform the pivotal relabelling procedure described in Egidi et al. (2018) and to obtain the relabelled posterior estimates.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
piv_rel(mcmc, nMC)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{mcmc}] The output of the MCMC sampling from \code{piv\_MCMC}.

\item[\code{nMC}] The number of total MCMC iterations (given in input to the \code{piv\_MCMC} function, or any function suited for MCMC sampling).
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Prototypical models in which the label switching problem arises
are mixture models, as explained in the Details section of
the \code{piv\_MCMC} function.

These models are unidentified with respect to an arbitrary permutation
of the labels \eqn{1,...,k}{}. Relabelling means permuting
the labels at each iteration of the Markov chain in such
a way that the relabelled chain can be used to draw inferences
on component-specific parameters.


We assume here that a MCMC sample is obtained for the
posterior distribution of a Gaussian mixture model--for instance via
\code{piv\_MCMC} function--with a prior distribution which is
labelling invariant.
Furthermore, suppose that we can find \eqn{k}{} units, one
for each group, which are (pairwise) separated with (posterior)
probability one
(that is, the posterior probability of any two of them being
in the same group
is zero).
It is then straightforward to use the \eqn{k}{} units,
called pivots in what follows and denoted by the indexes
\eqn{i_1,\ldots,i_k}{}, to identify the groups and to
relabel the chains:
for each MCMC iteration \eqn{h=1,\ldots, H}{} (\eqn{H}{} corresponds to
the argument \code{nMC}) and group
\eqn{j=1,\ldots,k}{}, set
\deqn{
[\mu_j]_h=[\mu_{[Z_{i_{j}}]_h}]_h;
}{}
\deqn{
[Z_{i}]_h=j \mbox{ for } i:[Z_i]_h=[Z_{i_{j}}]_h.
}{}
The applicability of this strategy is limited by the existence of the pivots,
which is not guaranteed. The existence of the pivots is a requirement of the
method, meaning that its use is restricted to those chains—or
those parts of a chain—for which the pivots are present. First, although the
model is based on a mixture of \eqn{k}{} components, each iteration of the chain
may imply a different number of non-empty groups. Let then \eqn{[k]_h \leq k}{}
be the number of non-empty groups at iteration \eqn{h}{},
\deqn{
 [k]_h = \#\{j: [Z_i]_h=j\mbox{ for some }i\},
}{}
where \eqn{\#A}{} is the cardinality of the set \eqn{A}{}. Hence, the relabelling
procedure outlined above can be used only for the subset of the chain
for which \eqn{[k]_h=k}{}; let it be \deqn{\mathcal{H}_k=\{h:[k]_h= k\},}{}
which correspond to the argument \code{true.iter} given by \code{piv\_MCMC}.
This means that the resulting relabelled chain is not a sample (of size \eqn{H}{})
from the posterior distribution, but a sample (of size \eqn{\#\mathcal{H}_k}{})
from the posterior
distribution conditional on there being (exactly) \eqn{k}{} non-empty groups.
Even if \eqn{k}{} non-empty groups are available, however,
there may not be \eqn{k}{} perfectly separated units. Let us define
\deqn{
 \mathcal{H}^{*}_k=\{ h\in\mathcal{H}_k : \exists r,s \mbox{ s.t. }
 [Z_{i_r}]_h=[Z_{i_s}]_h \}}{}

that is, the set of iterations where (at least) two pivots are in the same
group.
In order for the pivot method to be applicable,
we need to exclude iterations \eqn{\mathcal{H}^{*}_k}{};
that is, we can perform the pivot relabelling on \eqn{\mathcal{H}_k-
\mathcal{H}^{*}_{k}}{}, corresponding to the argument \code{Final\_It}.
\end{Details}
%
\begin{Value}
This function gives the relabelled posterior estimates--both mean and medians--obtained from the Markov chains of the MCMC sampling.

\begin{ldescription}
\item[\code{\code{mu\_rel\_mean}}]  \code{k}-vector (in case of univariate mixture)
or \code{k x 2}
matrix (in case of bivariate mixture) of estimated posterior means for the mean parameters.
\item[\code{\code{mu\_rel\_median}}]  \code{k}-vector (in case of univariate mixture)
or \code{k x 2}
matrix (in case of bivariate mixture) of estimated posterior medians for the mean parameters.
\item[\code{\code{mu\_rel}}] Complete relabelled chains
\item[\code{\code{Final\_It}}] The final number of valid MCMC iterations,
as explained in Details
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Leonardo Egidi \url{legidi@units.it}
\end{Author}
%
\begin{References}\relax
Egidi, L., Pappada, R., Pauli, F. and Torelli, N. (2018). Relabelling in Bayesian Mixture
Models by Pivotal Units. Statistics and Computing, 28(4), 957-969.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}

#Univariate simulation

N   <- 250
nMC <- 2500
k   <- 3
p   <- rep(1/k,k)
x   <- 3
stdev <- cbind(rep(1,k), rep(200,k))
Mu    <- seq(-trunc(k/2)*x,trunc(k/2)*x,length=k)
W     <- c(0.2,0.8)
sim   <- piv_sim(N,k,Mu,stdev,W=W)
res   <- piv_MCMC(y = sim$y, k =k, nMC = nMC)
rel   <- piv_rel(mcmc=res, nMC = nMC)


#Bivariate simulation

N <- 200
k <- 3
nMC <- 5000
M1  <- c(-.5,8)
M2  <- c(25.5,.1)
M3  <- c(49.5,8)
Mu  <- matrix(rbind(M1,M2,M3),c(k,2))
stdev <- cbind(rep(1,k), rep(200,k))
Sigma.p1 <- matrix(c(stdev[1,1],0,0,stdev[1,1]),
                   nrow=2, ncol=2)
Sigma.p2 <- matrix(c(stdev[1,2],0,0,stdev[1,2]),
                   nrow=2, ncol=2)
W <- c(0.2,0.8)
sim <- piv_sim(N,k,Mu,stdev,Sigma.p1,Sigma.p2,W)
res <- piv_MCMC(y = sim$y, k = k, nMC = nMC)
rel <- piv_rel(mcmc = res, nMC = nMC)
piv_plot(y=sim$y, mcmc=res, rel_est = rel, type="chains")
piv_plot(y=sim$y, mcmc=res, rel_est = rel,
         type="hist")


\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{piv\_sel}{Pivotal Selection via Co-Association Matrix}{piv.Rul.sel}
%
\begin{Description}\relax
Finding the pivots according to three different
methods involving a co-association matrix C.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
piv_sel(C, clusters)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{C}] A \eqn{N \times N}{} co-association matrix, i.e.
a matrix whose elements are co-occurences of pair of units
in the same cluster among \eqn{H}{} distinct partitions.

\item[\code{clusters}] A vector of integers indicating
a partition of the \eqn{N}{} units into, say, \eqn{k}{} groups.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Given a set of \eqn{N}{} observations \eqn{(y_{1},y_{2},...,y_{N})}{}
(\eqn{y_i}{} may be a \eqn{d}{}-dimensional vector, \eqn{d \geq 1}{},
consider clustering methods to obtain \eqn{H}{} distinct partitions
into \eqn{k}{} groups.
The matrix \code{C} is the co-association matrix,
where \eqn{c_{i,p}=n_{i,p}/H}{}, with \eqn{n_{i,p}}{} the number of times
the pair \eqn{(y_{i},y_{p})}{} is assigned to the same
cluster among the \eqn{H}{} partitions.

Let \eqn{j}{} be the group containing units \eqn{\mathcal J_j}{},
the user may choose \eqn{{i^*}\in\mathcal J_j}{} that
maximizes one of the quantities:
\deqn{
 \sum_{p\in\mathcal J_j} c_{{i^*}p}}{}

or
\deqn{\sum_{p\in\mathcal J_j} c_{{i^*}p} - \sum_{j\not\in\mathcal J_j} c_{{i^*}p}.
}{}

These methods give the unit that maximizes the global
within similarity (\code{"maxsumint"} and the unit that
maximizes the difference between global within and
between similarities \code{"maxsumdiff"}, respectively.
Alternatively, we may choose \eqn{i^{*} \in\mathcal J_j}{}, which minimizes:
\deqn{\sum_{p\not\in\mathcal J_j} c_{i^{*}p},}{}
obtaining the most distant unit among the members
that minimize the global dissimilarity between one group
and all the others (\code{"maxsumnoint"}).
See the vignette for further details.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{\code{pivots}}]  A matrix with \eqn{k}{} rows and three
columns containing the indexes of the pivotal units for each method.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Leonardo Egidi \url{legidi@units.it}
\end{Author}
%
\begin{References}\relax
Egidi, L., Pappadà, R., Pauli, F. and Torelli, N. (2018). Relabelling in Bayesian Mixture
Models by Pivotal Units. Statistics and Computing, 28(4), 957-969.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
# Iris data

data(iris)
x<- iris[,1:4]
N <- length(iris[,1])
H <- 1000
a <- matrix(NA, H, N)

# Perform H k-means partitions

for (h in 1:H){
 a[h,] <- kmeans(x, centers = 3)$cluster
}
# Build the co-association matrix

C <- matrix(1, N,N)
for (i in 1:(N-1)){
 for (j in (i+1):N){
   C[i,j] <- sum(a[,i]==a[,j])/H
   C[j,i] <- C[i,j]
 }}

km <- kmeans(x, centers =3)

# Find the pivots according to the three possible pivoyal criterion

ris <- piv_sel(C, clusters = km$cluster)

plot(iris[,1], iris[,2], xlab ="Sepal.Length", ylab= "Sepal.Width",
col = km$cluster)

 # Add the pivots according to maxsumdiff criterion

points( x[ris$pivot[,3], c( "Sepal.Length","Sepal.Width" )], col = 1:3,
cex =2, pch = 8 )

\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{piv\_sim}{Generate Data from a Gaussian Nested Mixture}{piv.Rul.sim}
%
\begin{Description}\relax
Simulate N observations from a nested Gaussian mixture model
with k pre-specified components under uniform group probabilities \eqn{1/k}{},
where each group is in turn
drawn from a further level consisting of two subgroups.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
piv_sim(N, k, Mu, stdev, Sigma.p1 = matrix(c(1, 0, 0, 1), 2, 2, byrow =
  TRUE), Sigma.p2 = matrix(c(100, 0, 0, 100), 2, 2, byrow = TRUE),
  W = c(0.5, 0.5))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{N}] The desired sample size.

\item[\code{k}] The desired number of mixture components.

\item[\code{Mu}] The input mean vector/matrix.

\item[\code{stdev}] A \code{k x2} matrix of input standard deviations,
one for each group (from 1 to \code{k}) and for each subgroup (from 1 to 2).
For univariate mixtures only.

\item[\code{Sigma.p1}] The covariance matrix for the first subgroup. For bivariate mixtures only.

\item[\code{Sigma.p2}] The covariance matrix for the second subgroup. For bivariate mixtures only.

\item[\code{W}] The vector for the mixture weights of the two subgroups,
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The functions allows to simulate values from a double (nested) univariate
Gaussian mixture:

\deqn{
(Y_i|Z_i=j) \sim \sum_{s=1}^{2} p_{js}\, \mathcal{N}(\mu_{j}, \sigma^{2}_{js}),
}{}

or from a bivariate nested Gaussian mixture:

\deqn{
(Y_i|Z_i=j) \sim \sum_{s=1}^{2} p_{js}\, \mathcal{N}_{2}(\bm{\mu}_{j}, \Sigma_{s}),
}{}

where \eqn{\sigma^{2}_{js}}{} is the variance for the group \eqn{j}{} and
the subgroup \eqn{s}{} (\code{stdev} is the
argument for specifying the \eqn{k x 2}{} standard deviations
for univariate mixtures);
\eqn{\Sigma_s}{} is the covariance matrix for the
subgroup \eqn{s, s=1,2}{}, where the two matrices are
specified by \code{Sigma.p1}
and \code{Sigma.p2} respectively; \eqn{\mu_j}{} and
\eqn{\bm{\mu}_j, \ j=1,\ldots,k}{}
are the mean input vector and matrix respectively,
specified by the argument \code{Mu};
\code{W} is a vector of dimension 2 for the subgroups weights.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{\code{y}}] The \code{N} simulated observations.
\item[\code{\code{true.group}}] A vector of integers from \code{1:k}
indicating the values of the latent variables \eqn{Z_i}{}.
\item[\code{\code{subgroups}}] A \code{2 x N} matrix with values 1 or 2
indicating the subgroup to which each observation is drawn from.
\end{ldescription}
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}

# Bivariate mixture simulation with three components

N  <- 2000
k  <- 3
M1 <- c(-45,8)
M2 <- c(45,.1)
M3 <- c(100,8)
Mu <- matrix(rbind(M1,M2,M3),c(k,2))
stdev    <- cbind(rep(1,k), rep(200,k))
Sigma.p1 <- matrix(c(stdev[1,1],0,0,stdev[1,1]),
nrow=2, ncol=2)
Sigma.p2 <- matrix(c(stdev[1,2],0,0,stdev[1,2]),
 nrow=2, ncol=2)
W   <- c(0.2,0.8)
sim <- piv_sim(N, k, Mu, Sigma.p1 = Sigma.p1,
Sigma.p2 = Sigma.p2, W)
plot(sim$y, xlab="y[,1]", ylab="y[,2]")

\end{ExampleCode}
\end{Examples}
\printindex{}
\end{document}
