% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bayes_mcmc.R
\name{piv_MCMC}
\alias{piv_MCMC}
\title{JAGS Sampling for Gaussian Mixture Models and Clustering via Co-Association Matrix.}
\usage{
piv_MCMC(y, k, nMC, piv.criterion, clustering)
}
\arguments{
\item{y}{N-dimensional data vector/matrix.}

\item{k}{Number of mixture components.}

\item{nMC}{Number of MCMC iterations for the JAGS function execution.}

\item{piv.criterion}{The pivotal method used for detecting the pivots, one for each group. Possible choices: \code{maxsumint}, \code{maxsumnoint}, \code{maxsumdiff}, \code{MUS}. \code{MUS} is available for \code{k<5}. If \code{piv.criterion=NULL}, \code{maxsumdiff} is chosen by default. See \code{Details} for a thorough and detailed list of available pivotal methods.}

\item{clustering}{The clustering technique adopted for partitioning the \code{N} observations into \code{k} groups. possible choices: \code{diana} (default), \code{hclust}.}
}
\value{
The function gives the MCMC output, the clustering solutions and the pivotal indexes. Here is a complete list of outputs.

\item{\code{Freq}}{  Number of units corresponding to each group for the post-processed chains.}
\item{\code{z} }{  Post-processed latent vector.}
\item{\code{ris}}{  MCMC output array as provided by JAGS.}
\item{\code{groupPost}}{ Post-processed group vector.}
\item{ \code{mu_switch}}{  Post-processed MCMC chains for the mean parameters.}
\item{\code{ mu_pre_switch_compl}}{ Pre-precessed MCMC chains for the mean parameters.}
\item{\code{C}}{Co-association matrix constructed from the MCMC sample.}
\item{\code{grr}}{Group vector allocation as provided by \code{diana} or \code{hclust}.}
\item{\code{clust_sel}}{clustering solution obtained via \code{diana} or \code{hclust} function.}
\item{\code{true.iter}}{ The number of MCMC iterations for which their number of groups exactly coincides with the prespecified number of groups \code{k}. }
}
\description{
Perform MCMC JAGS sampling for Gaussian mixture models, post-process the chains and apply a clustering technique to the MCMC sample. Pivotal units for each group are selected among four alternative criteria.
}
\details{
The function fits a Bayesian mixture model of the form:
\deqn{(Y_i|Z_i=j) \sim f(y;\mu_j,\phi),}
where the \eqn{Z_i}, \eqn{i=1,\ldots,n}, are i.i.d. random variables, \eqn{j=1,\dots,k}, \eqn{\phi} is a parameter which is common to all components,  \eqn{Z_i\in\{1,\ldots,k \}}, and
\deqn{P(Z_i=k)=\pi_k.}
The likelihood of the model is then
\deqn{L(\vy;\vmu,\vpi,\phi) = \prod_{i=1}^n \sum_{j=1}^k \pi_k f(y_i;\mu_k,\phi),}
with \eqn{\vmu=(\mu_{1},\dots,\mu_{k})} component-specific parameters and \eqn{\vpi=(\pi_{1},\dots,\pi_{k})} mixture weights. Let \eqn{\nu} denote a permutation of \eqn{\{ 1,\ldots,k \}}, and let \eqn{\nu(\bm{\mu})= (\mu_{\nu(1)},\ldots,} \eqn{ \mu_{\nu(k)})}, \eqn{ \nu(\bm{\pi})=(\pi_{\nu(1)},\ldots,\pi_{\nu(k)})} be the corresponding permutations of \eqn{\vmu} and \eqn{\vpi}. Denote by \eqn{\mathcal{V}} the set of all the permutations of the indexes \eqn{\{1,\ldots,k \}}, the likelihood above is invariant under any permutation \eqn{\nu \in \mathcal{V}}, that is
\deqn{
L(\vy;\vmu,\vpi,\phi) = L(\vy;\nu(\bm{\mu}),\nu(\bm{\pi}),\phi).}
As a consequence, the model is unidentified with respect to an arbitrary permutation of the labels.
When Bayesian inference for the model is performed, if the prior distribution \eqn{p_0(\vmu,\vpi,\phi)} is invariant under a permutation of the indices, then so is the posterior. That is, if \eqn{p_0(\vmu,\vpi,\phi) = p_0(\nu(\bm{\mu}),\nu(\bm{\pi}),\phi)}, then
\deqn{
p(\vmu,\vpi,\phi|\vy) \propto p_0(\vmu,\vpi,\phi)L(\vy;\vmu,\vpi,\phi)}
is multimodal with (at least) \eqn{k!} modes.
The function performs JAGS sampling using the \code{bayesmix} package for univariate Gaussian mixtures, and \code{runjags} for bivariate Gaussian mixtures. After MCMC sampling,
this function calls the \code{pivotal_selection()} function and yields the pivots obtained from one among four different
methods: \code{maxsumint}, \code{maxsumnoint}, \code{maxsumdiff} and \code{MUS} (available only if \code{k < 5}).
Denoting woth \eqn{C} a co-association matrix counting the number of times a pair of units belongs to the same group  across the MCMC sample, and let
\eqn{j} be the group containing units \eqn{\mathcal J_j}, the user may chose \eqn{{i^*}\in\mathcal J_j} that maximizes one of the quantities
\deqn{\sum_{p\in\mathcal J_j} c_{{i^*}p};  \quad }
\deqn{ \sum_{p\in\mathcal J_j} c_{{i^*}p} - \sum_{j\not\in\mathcal J_j} c_{{i^*}p}.}
These methods give the unit that maximizes the global within similarity (\code{maxsumint}) and the unit that maximizes the difference between global within and between similarities (\code{maxsumdiff}), respectively. Alternatively, we may choose \eqn{i^{*} \in\mathcal J_j}, which minimizes:
 \deqn{\sum_{p\not\in\mathcal J_j} c_{ip}, }
obtaining the most distant unit among the members that minimize the global dissimilarity between one group and all the others (\code{maxsumnoint}). The function \code{MUS} calls the MUS procedure when \code{k<5}.
}
\examples{
N   <- 200
k   <- 4
nMC <- 1000
M1  <-c(-.5,8)
M2  <- c(25.5,.1)
M3  <- c(49.5,8)
M4  <- c(63.0,.1)
Mu  <- matrix(rbind(M1,M2,M3,M4),c(4,2))
stdev    <- cbind(rep(1,k), rep(200,k))
Sigma.p1 <- matrix(c(stdev[1,1],0,0,stdev[1,1]), nrow=2, ncol=2)
Sigma.p2 <- matrix(c(stdev[1,2],0,0,stdev[1,2]), nrow=2, ncol=2)
W <- c(0.2,0.8)
sim <- piv_sim(N,k,Mu, stdev, Sigma.p1,Sigma.p2,W)
res <- piv_MCMC(sim$y, k, nMC)


Fishery data

data(fish)
y <- fish[,1]
k <- 5
nMC <- 5000
res <- bayesMCMC(y, k, nMC)
}
\references{
Egidi, L., Pappada, R., Pauli, F. and Torelli, N. (2018). Relabelling in Bayesian Mixture
Models by Pivotal Units. Statistics and Computing, 28(4), 957-969, DOI 10.1007/s11222-017-  9774-2.
}
\author{
Leonardo Egidi \url{legidi@units.it}
}
